{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Herz Erkrankungen - Wer ist erkrankt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![heart_desease.png](heart_desease.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bibliotheken importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas seaborn matplotlib numpy torch sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Daten laden\n",
    "\n",
    "Datenquelle: https://archive.ics.uci.edu/dataset/45/heart+disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"heart.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Erster Einblick in Daten\n",
    "\n",
    "- gewinne eine erste Übersicht über die Date\n",
    "- schaue, ob eventuell Daten fehlen\n",
    "- wie sind die Daten verteilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Hinweis</b></summary>\n",
    "  <p><code>.head(n)</code> (n=Anzahl der Spalten): zeigt die ersten n Spalten des DataFrames</p>\n",
    "  <p><code>.info()</code>: Übersicht des DataFrames (Anzahl der Einträge, Spaltennamen, Datentypen, Speicherverbrauch)</p>\n",
    "  <p><code>.describe()</code>: statistische Übersicht für numerische Spalten (Mittelwert, Standardabweichung, Minimum, Maximum)</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1: Schaue dir die Metainformationen an\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2: Schaue dir die Statistik der Daten an\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Hinweis</b></summary>\n",
    "  <p><code>.head(n)</code> (n=Anzahl der Spalten): zeigt die ersten n Spalten des DataFrames</p>\n",
    "  <p><code>.info()</code>: Übersicht des DataFrames (Anzahl der Einträge, Spaltennamen, Datentypen, Speicherverbrauch)</p>\n",
    "  <p><code>.describe()</code>: statistische Übersicht für numerische Spalten (Mittelwert, Standardabweichung, Minimum, Maximum)</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenbeschreibung\n",
    "\n",
    "| Feature   | Description | Values |\n",
    "|-----------|-------------|--------|\n",
    "| age       | Age in years | - |\n",
    "| sex       | Sex | 1 = male, 0 = female |\n",
    "| cp        | Chest pain type | 0 = typical angina, 1 = atypical angina, 2 = non-anginal pain, 3 = asymptomatic |\n",
    "| trestbps  | Resting blood pressure (in mm Hg on admission to the hospital) | - |\n",
    "| chol      | Serum cholesterol in mg/dl | - |\n",
    "| fbs       | Fasting blood sugar > 120 mg/dl | 1 = true, 0 = false |\n",
    "| restecg   | Resting electrocardiographic results | 0 = normal, 1 = ST-T wave abnormality, 2 = probable or definite left ventricular hypertrophy by Estes' criteria |\n",
    "| thalach   | Maximum heart rate achieved | - |\n",
    "| exang     | Exercise-induced angina | 1 = yes, 0 = no |\n",
    "| oldpeak   | ST depression induced by exercise relative to rest | - |\n",
    "| slope     | Slope of the peak exercise ST segment | 0 = upsloping, 1 = flat, 2 = downsloping |\n",
    "| ca        | Number of major vessels colored by fluoroscopy | 0–3 |\n",
    "| thal      | Thallium stress test result | 0 = error/NaN, 1 = fixed defect, 2 = normal, 3 = reversible defect |\n",
    "| target    | Diagnosis of heart disease | 0 = no disease, 1 = disease |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Folgerung</b></summary>\n",
    "  <ul>\n",
    "    <li><b>data[\"ca\"]:</b> enthält Werte > 3, die inkorrekt sind. Diese Werte sollten entfernt werden.</li>\n",
    "    <li><b>data[\"thal\"]:</b> enthält Werte = 0, die inkorrekt sind. Diese Werte sollten ebenfalls entfernt werden.</li>\n",
    "  </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['ca'] < 4]\n",
    "data = data[data['thal'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wissenseinschub - Thema und Daten verstehen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Informationsraster</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            display: flex;\n",
    "            justify-content: center;\n",
    "            align-items: center;\n",
    "            background-color: #f4f4f4;\n",
    "            margin: 0;\n",
    "            padding: 5vw; /* Dynamischer Abstand basierend auf der Bildschirmbreite */\n",
    "            box-sizing: border-box;\n",
    "            color: black;\n",
    "        }\n",
    "        .grid-container {\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(2, 1fr);\n",
    "            grid-template-rows: repeat(3, 1fr);\n",
    "            gap: 5vw; /* Dynamischer Abstand zwischen den Grid-Items */\n",
    "            width: 100%;\n",
    "            max-width: 100%; /* Container passt sich der Bildschirmbreite an */\n",
    "            padding: 2vw; /* Dynamischer Innenabstand basierend auf der Bildschirmbreite */\n",
    "            box-sizing: border-box;\n",
    "        }\n",
    "        .grid-item {\n",
    "            background: white;\n",
    "            padding: 2vw; /* Dynamischer Innenabstand basierend auf der Bildschirmbreite */\n",
    "            border-radius: 10px;\n",
    "            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
    "            text-align: center;\n",
    "            display: flex;\n",
    "            flex-direction: column;\n",
    "            justify-content: space-between;\n",
    "        }\n",
    "        .grid-item img {\n",
    "            width: 100%; /* Bildbreite passt sich der Breite des Grid-Items an */\n",
    "            height: auto; /* Höhe wird automatisch angepasst, um das Seitenverhältnis beizubehalten */\n",
    "            object-fit: contain; /* Bild wird skaliert, um den Container zu füllen, ohne zugeschnitten zu werden */\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        h3 {\n",
    "            margin-bottom: 2vw; /* Dynamischer Abstand basierend auf der Bildschirmbreite */\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <div class=\"grid-container\">\n",
    "        <div class=\"grid-item\">\n",
    "            <h3>Blutdruck - trestbps</h3>\n",
    "            <p>In diesem Datensatz ist nur \n",
    "                <a href=\"https://www.verywellhealth.com/systolic-and-diastolic-blood-pressure-1746075\">\n",
    "                    systolischer Blutdruck\n",
    "                </a> \n",
    "                enthalten.\n",
    "            </p>\n",
    "            <img src=\"Blutdruck.jpg\" alt=\"Blutdruck\" style=\"height: 40vw;\">\n",
    "        </div>\n",
    "        <div class=\"grid-item\">\n",
    "            <h3>Cholesterin - chol</h3>\n",
    "            <p>LDL (schlechtes Cholesterin), HDL (gutes Cholisterin)</p>\n",
    "            <img src=\"cholesterin.jpeg\" alt=\"Cholesterin\">\n",
    "        </div>\n",
    "        <div class=\"grid-item\">\n",
    "            <h3>Brustschmerzen - cp</h3>\n",
    "            <p>typische Angia (0): Druck/Engegefühl in Brust; meist bei Anstrengung; bessert sich in Ruhe</p>\n",
    "            <p>atypische Angia (1): ungewöhnliche Symptome; Schmerzen an anderen Stellen oder in Ruhe</p>\n",
    "            <p>nicht-anginöser Schmerz (2): Schmerzen, die nicht auf Herz zurückzuführen sind; Muskel-/Verdauungsprobleme</p>\n",
    "            <p>asymptomatisch (3): keine spürbaren Brustschmerzen; oft bei stillen Herzroblemen</p>\n",
    "            <img src=\"angina.png\" alt=\"Angina\">\n",
    "        </div>\n",
    "        <div class=\"grid-item\">\n",
    "            <h3>Ruhe-Elektrodiografie (EKG) - restecg</h3>\n",
    "            <p>normal (0): keine Auffälligkeiten; Herzrhytmus und elektrische Aktivität im Normalbereich</p>\n",
    "            <p>ST-T-Wellen-Abnomalie (1): Veränderung in ST- oder T-Wellen; deutet auf Ischämie (unzureichende Blutversorgung des Herzmuskels), Elektrolytstörung oder andere kardiale Probleme hin</p>\n",
    "            <p>wahrscheinliche oder definitive linksventrikuläre Hypertrophie nach Estes´ Kriterien (2): Merkmale in EKG deuten auf Verdickung linker Herzkammer hin; oft durch Bluthochdruck oder andere herzerkrankungen</p>\n",
    "            <img src=\"EKG.png\" alt=\"EKG\">\n",
    "        </div>\n",
    "        <div class=\"grid-item\">\n",
    "            <h3>Fluoroskopie - ca</h3>\n",
    "            <p>Kontrastmittel in Hauptgefäßen um Verengungen oder Blockaden zu identifizieren</p>\n",
    "            <img src=\"Fluoroscopy.png\" alt=\"Fluoroskopie\">\n",
    "        </div>\n",
    "        <div class=\"grid-item\">\n",
    "            <h3>Thalium-Stresstest - thal</h3>\n",
    "            <p>Fehler/NaN (0)</p>\n",
    "            <p>fester Defekt (1): in Ruhe und Belastund verminderte Durchblutung; deutet auf Narbengewebe durch früheren Herzinfarkt hin</p>\n",
    "            <p>Normal (2)</p>\n",
    "            <p>reversibler Defekt (3): ein Bereich des Herzmuskels unter Belastung verminderte Durchblutung (in Ruhe normalisiert); oft durch Verengung der Koronararterie</p>\n",
    "            <img src=\"Thalium.jpeg\" alt=\"Thalium-Test\">\n",
    "        </div>\n",
    "    </div>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gewinne einen detailierten Einblick in die Daten\n",
    "- stelle Verteilungen, Außreißer und Korrelationen grafisch dar\n",
    "\n",
    "Dokumentation Matplotlib: https://matplotlib.org/stable/plot_types/index </br>\n",
    "Dokumentation Seaborn: https://seaborn.pydata.org/examples/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Verteilung von Werten einzelner Features: </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Hinweis zur Datenvisualisierung</b></summary>\n",
    "  <br>\n",
    "  In der Datenanalyse ist es wichtig, zwischen kategorialen und kontinuierlichen Features zu unterscheiden, da sie unterschiedliche Visualisierungstechniken erfordern:\n",
    "\n",
    "  - **Kategoriale Daten** sind Daten, die in Gruppen oder Kategorien unterteilt sind, z. B. Geschlecht (männlich/weiblich) oder Farbe (rot/blau/grün). Diese Daten können als diskrete Werte betrachtet werden.\n",
    "  - **Kontinuierliche Daten** sind Daten, die in einem kontinuierlichen Bereich existieren, z. B. Temperatur oder Gewicht. Sie können jeden Wert innerhalb eines Intervalls annehmen und werden oft auf einer kontinuierlichen Skala dargestellt.\n",
    "\n",
    "  **Warum diese Unterscheidung wichtig ist:**\n",
    "  - Kategoriale und kontinuierliche Daten haben unterschiedliche Eigenschaften und erfordern daher unterschiedliche Visualisierungsmethoden, um Muster und Zusammenhänge klar darzustellen.\n",
    "  \n",
    "  **Visualisierung mit Seaborn und Matplotlib:**\n",
    "\n",
    "  - **Für kategoriale Daten:**\n",
    "    - **Piecharts** sind nützlich, um die prozentuale Verteilung von Kategorien darzustellen. Sie zeigen, wie sich verschiedene Kategorien im Verhältnis zueinander verhalten.\n",
    "      - Beispiel mit Matplotlib:\n",
    "        ```python\n",
    "        import matplotlib.pyplot as plt\n",
    "        data = ['Kategorie A', 'Kategorie B', 'Kategorie C']\n",
    "        values = [30, 40, 30]\n",
    "        plt.pie(values, labels=data, autopct='%1.1f%%', startangle=90)\n",
    "        plt.title('Verteilung der Kategorien')\n",
    "        plt.show()\n",
    "        ```\n",
    "      - Beispiel mit Seaborn (für eine ähnliche Darstellung):\n",
    "        ```python\n",
    "        import seaborn as sns\n",
    "        sns.countplot(x='Kategorie', data=df)\n",
    "        plt.title('Verteilung der Kategorien')\n",
    "        plt.show()\n",
    "        ```\n",
    "\n",
    "  - **Für kontinuierliche Daten:**\n",
    "    - **Distplots (Verteilungsdiagramme)** und **Boxplots** sind hilfreich, um die Verteilung, Ausreißer und zentrale Tendenzen von kontinuierlichen Daten zu visualisieren.\n",
    "      - **Distplot** (mit optionaler KDE) zeigt die Häufigkeit der Werte und deren Verteilung:\n",
    "        ```python\n",
    "        import seaborn as sns\n",
    "        sns.histplot(df['Wert'], kde=True)\n",
    "        plt.title('Verteilung der kontinuierlichen Daten')\n",
    "        plt.show()\n",
    "        ```\n",
    "      - **Boxplot** zeigt die quartilebasierte Verteilung und identifiziert Ausreißer:\n",
    "        ```python\n",
    "        sns.boxplot(x=df['Wert'])\n",
    "        plt.title('Boxplot der kontinuierlichen Daten')\n",
    "        plt.show()\n",
    "        ```\n",
    "\n",
    "  <br>\n",
    "  <b>Zusammenfassung:</b>\n",
    "  <ul>\n",
    "    <li>Kategoriale Daten sind für Piecharts oder Countplots geeignet, um die Häufigkeit der Kategorien darzustellen.</li>\n",
    "    <li>Kontinuierliche Daten werden am besten mit Distplots oder Boxplots visualisiert, um die Verteilung, Ausreißer und zentrale Tendenzen zu zeigen.</li>\n",
    "  </ul>\n",
    "\n",
    "  <br><img src=\"dist-and-outliers.png\" alt=\"Verteilung und Ausreißer\" style=\"max-width: 100%; height: auto;\"/>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A3: Definiere 2 Arrays, die kategeriale und kontinuierliche Features umfassen\n",
    "\n",
    "categorical_features = \n",
    "continuous_features = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A4: Stelle die Verteilung der Features dar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>📌 Wichtige Erkenntnisse</b></summary>\n",
    "  <br>\n",
    "  <ul>\n",
    "    <li><b>Ungleichverteilung der Daten:</b> \n",
    "      <br> Sowohl kategoriale als auch kontinuierliche Features sind nicht gleichmäßig verteilt. \n",
    "      <br> Dies kann das Training neuronaler Netzwerke beeinflussen.</li>\n",
    "    <br>\n",
    "    <li><b>Neuronale Netzwerke sind skalenempfindlich:</b> \n",
    "      <br> Unterschiedliche Wertebereiche der Features können das Lernen erschweren. \n",
    "      <br><b>➜ Lösung:</b> Standardisierung oder Normalisierung der Daten.</li>\n",
    "    <br>\n",
    "    <li><b>Ausreißer in kontinuierlichen Features:</b> \n",
    "      <br> Werte außerhalb des folgenden Bereichs gelten als Ausreißer:\n",
    "      <ul>\n",
    "        <li><b>Untere Grenze:</b> Q1 − 1.5 × IQR</li>\n",
    "        <li><b>Obere Grenze:</b> Q3 + 1.5 × IQR</li>\n",
    "      </ul>\n",
    "      <br><b>➜ Erklärung:</b> Der Interquartilsabstand (IQR) beschreibt die mittleren 50 % der Daten.\n",
    "      Werte außerhalb dieses Bereichs sind potenzielle Ausreißer.</li>\n",
    "  </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Korrelation und Zusammenhang zwischen Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Hinweis</b></summary>\n",
    "  <p><code>.corr()</code>: Berechnet <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\">Korrelationsmatrix</A> aller Features</p>\n",
    "  <p><code>sns.heatmap()</code>: Eine <a href=\"https://seaborn.pydata.org/generated/seaborn.heatmap.html\"> Heatmap </a> bietet einen intuitiven und schnellen Einblick in Werte mittels Farbcodierung</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A5: Stelle die Korrelation zwischen Target und den anderen Features grafisch dar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Datenaufbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Daten aufteilen\n",
    "\n",
    "Zuerst müssen die Daten in die Merkmale (X) und die Zielvariable (y) aufgeteilt werden. </br>\n",
    "Um das Model später trainieren zu können und unser Ergebnis, an dem Model unbekannten Daten, zu testen müssen die Daten in Trainings-, Test- und Validierungsdaten aufgeteilt werden.\n",
    "Zuerst die Aufteilung in Trainings und Testdaten um danach aus den Trainingsdaten Validierungsdaten zu extrahieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Hinweis</b></summary>\n",
    "  <p><code>.drop(\"target\")</code>: Entfernt die Variable \"target\" aus den Features</p>\n",
    "  <p><code>df[\"target\"]</code>: Gibt die Spalte \"target\" zurück</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A6: Unterteile die Daten in Merkmale (X) und Zielvariable (y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Hinweis</b></summary>\n",
    "  <p><code>train_test_split(dataframe1, dateframe2, test_size=0.x, random_stat=42)</code>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">train_test_split</a> teilt die Dataframes nach mit der angegebenen Aufteilung auf</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A7: Unterteile X und y in train, test und val Daten\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Daten skalieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Skalierung von Daten und der StandardScaler</b></summary>\n",
    "  <p>Die <b>Skalierung</b> von Daten ist ein entscheidender Schritt in der Vorverarbeitung von Daten für maschinelles Lernen. Sie sorgt dafür, dass alle Merkmale (Features) auf einem ähnlichen Wertebereich liegen, was für viele Algorithmen wichtig ist, da diese oft empfindlich gegenüber der Skala der Eingabedaten sind.</p>\n",
    "\n",
    "  <p><b>Warum Skalierung notwendig ist:</b></p>\n",
    "  <ul>\n",
    "    <li>Algorithmen wie <i>Gradient Descent</i> und <i>k-NN</i> sind empfindlich gegenüber der Skala der Eingabedaten. Wenn die Features unterschiedliche Skalen haben (z. B. ein Feature von 0 bis 1 und ein anderes von 0 bis 1000), kann das Modell Schwierigkeiten haben, die Daten zu verarbeiten und zu lernen.</li>\n",
    "    <li>Skalierung sorgt dafür, dass alle Features einen ähnlichen Einfluss auf das Modell haben, was das Training stabiler und schneller macht.</li>\n",
    "    <li>Beispiel: Wenn eines der Features eine viel größere Range hat, könnte das Modell dazu neigen, diesem Feature mehr Gewicht zu geben, obwohl es möglicherweise weniger wichtig ist.</li>\n",
    "  </ul>\n",
    "\n",
    "  <p><b>Was bewirkt die Skalierung?</b></p>\n",
    "  <ul>\n",
    "    <li>Die Skalierung verändert die Daten so, dass sie in einem bestimmten Bereich liegen oder eine bestimmte Verteilung haben. Häufig verwendete Skalierungsansätze sind die Standardisierung und Min-Max-Skalierung.</li>\n",
    "    <li>Durch Skalierung wird der Mittelwert eines Features auf einen bestimmten Wert (z. B. 0) gesetzt und die Standardabweichung auf einen bestimmten Wert (z. B. 1), was die Daten zentriert und skaliert.</li>\n",
    "  </ul>\n",
    "\n",
    "  <p><b>Wie funktioniert der StandardScaler?</b></p>\n",
    "  <p>Der <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\" target=\"_blank\"><b>StandardScaler</b></a> von <i>scikit-learn</i> standardisiert die Features, indem er den Mittelwert (Durchschnitt) jedes Features auf 0 und die Standardabweichung auf 1 setzt. Die Formel lautet:</p>\n",
    "\n",
    "  <pre>\n",
    "    Z = (X - μ) / σ\n",
    "  </pre>\n",
    "  <p>Dabei ist:</p>\n",
    "  <ul>\n",
    "    <li><b>X</b>: Der ursprüngliche Wert des Features.</li>\n",
    "    <li><b>μ</b>: Der Mittelwert des Features.</li>\n",
    "    <li><b>σ</b>: Die Standardabweichung des Features.</li>\n",
    "    <li><b>Z</b>: Der standardisierte Wert des Features.</li>\n",
    "  </ul>\n",
    "\n",
    "  <p>Der StandardScaler berechnet den Mittelwert und die Standardabweichung des Trainingsdatensatzes und wendet dann diese Werte auf die Trainings-, Validierungs- und Testdaten an (wobei er die Testdaten nur transformiert, nicht anpasst, um Datenleakage zu vermeiden).</p>\n",
    "\n",
    "  <p>Die Vorteile des StandardScalers sind, dass er die Daten zentriert und skaliert, was insbesondere für Modelle wie <i>lineare Regression</i>, <i>Support Vector Machines</i> und neuronale Netze von Bedeutung ist.</p>\n",
    "\n",
    "  <p>Weitere Informationen findest du in der <a href=\"https://scikit-learn.org/stable/modules/preprocessing.html\" target=\"_blank\">offiziellen Dokumentation von scikit-learn</a>.</p>\n",
    "\n",
    "  <br>\n",
    "  <img src=\"scaling.png\" alt=\"Scaling vorher vs. danach\" style=\"max-width: 100%; height: auto;\"/>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A8: Skaliere die Daten (X_train: fit_transform, X_val, X_test: transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model trainieren und auswerten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Datenvorbereitung:** Konvertierung von bereits skalierten Daten in PyTorch-Tensoren.\n",
    "2. **Modell-Definition:** Aufbau eines simplen Netzwerks mit zwei verborgenen Schichten.\n",
    "3. **Training:** Training des Netzwerks über 2500 Epochen mit Aufzeichnung des Trainingsverlusts.\n",
    "4. **Evaluation:** Berechnung von Accuracy, Precision und Recall auf den Validierungs- und Testdaten.\n",
    "5. **Visualisierung:** Darstellung des Trainingsverlaufs und der Evaluationsmetriken als Grafiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter definieren\n",
    "batch_size = 32\n",
    "input_size = 13  # Anzahl der Eingabefeatures\n",
    "output_size = 1  # Nur eine Ausgabe (Krankheit: Ja/Nein)\n",
    "hidden_layers = [32, 16]  # Zwei verborgene Schichten\n",
    "epochs = 1000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Daten in PyTorch-Tensoren umwandeln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ausgehend davon, dass die Daten bereits vorverarbeitet (z. B. skaliert) vorliegen, müssen wir sie in das Format bringen, das PyTorch für das Training benötigt. Dabei erfolgt Folgendes:\n",
    "\n",
    "- Tensor-Konvertierung: Die Feature-Matrizen und Zielvariablen werden in PyTorch-Tensoren umgewandelt. Dabei wird der Datentyp auf ```float32``` festgelegt. Für die Zielvariablen wird zusätzlich die Dimension angepasst, sodass sie als Spaltenvektor vorliegen (mittels ```.view(-1, 1)```).\n",
    "\n",
    "- Erstellung eines Datasets: Mit ```TensorDataset``` werden die zugehörigen Feature- und Ziel-Tensoren zusammengeführt, sodass jeder Eintrag im Dataset ein Tupel aus (Feature, Ziel) darstellt.\n",
    "\n",
    "- Erstellung eines DataLoaders: Mittels ```DataLoader``` wird das Dataset in Batches unterteilt, um das Training zu erleichtern. Beim Trainings-Dataloader wird ```shuffle=True``` verwendet, um die Daten vor jedem Trainingsepochendurchlauf zufällig zu mischen – dies hilft, die Generalisierungsfähigkeit des Modells zu verbessern. Für den Validierungs- (und Test-) Dataloader wird ```shuffle=False``` gewählt, um die Reihenfolge der Daten beizubehalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A9: Wandle die Daten in Pytorch-Tensoren um\n",
    "\n",
    "# Hier gehe ich davon aus, dass die Daten bereits skaliert sind\n",
    "X_train_tensor =\n",
    "y_train_tensor =\n",
    "\n",
    "X_val_tensor =\n",
    "y_val_tensor =\n",
    "\n",
    "X_test_tensor = \n",
    "y_test_tensor =\n",
    "\n",
    "# Erstelle DataLoader für das Training und die Validierung\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Modell definieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Das SimpleNN-Modell besteht aus einer Eingabeschicht, zwei verborgenen Schichten mit ReLU-Aktivierung und einer Ausgabeschicht, die eine Sigmoid-Aktivierung verwendet (ideal für binäre Klassifikation).\n",
    "<details>\n",
    "<summary><b>Hinweis</b></summary>\n",
    "<p><a href=\"https://playground.tensorflow.org/\">Hier</a> kannst du dier anschauen wie dein Netzwerk aussehen könnte un was die Parameter bewirken.</p>\n",
    "<p><a href=\"https://yajm.medium.com/simple-neural-network-in-pytorch-step-by-step-guide-fe12bdbbae33\">Hier</a> findest du eine simple Dokumentation zum erstellen eines neuronalen Netzwerkes.</p>\n",
    "</br>\n",
    "<p>Dies ist ein möglicher Aufbau des Netzwekrs:</p>\n",
    "<img src=\"simplenn_arch.png\" alt=\"SimpleNN Architektur\" style=\"max-width: 100%; height: auto;\"/>\n",
    "\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A10: Definiere das Modell\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        # Eingabeschicht\n",
    "        #...\n",
    "        # Verborgene Schichten\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            #...\n",
    "        # Ausgabeschicht\n",
    "        #...\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Modell instanziieren\n",
    "model = SimpleNN(input_size, hidden_layers, output_size)\n",
    "\n",
    "# Verlustfunktion und Optimierer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss für binäre Klassifikation\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Modell trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Code führt einen Trainingszyklus über eine festgelegte Anzahl an Epochen durch. Dabei werden in jeder Epoche die folgenden Schritte durchgeführt:\n",
    "\n",
    "1. Moduswechsel und Initialisierung: \n",
    "    - Das Modell wird in den Trainingsmodus versetzt (mit ```model.train()```), damit alle Schichten (z. B. Dropout, BatchNorm) korrekt arbeiten. \n",
    "    - Eine Variable (```running_loss```) wird initialisiert, um den kumulativen Verlust der aktuellen Epoche zu speichern.\n",
    "\n",
    "2. Batch-Weise Verarbeitung: \n",
    "    - Für jeden Batch aus dem Trainings-Dataloader wird ein Vorwärtsdurchlauf ausgeführt, indem das Modell die Eingaben verarbeitet und Vorhersagen (```outputs```) liefert.\n",
    "\n",
    "    - Anschließend wird der Verlust (Loss) zwischen den Vorhersagen und den tatsächlichen Zielwerten berechnet.\n",
    "\n",
    "    - Um die Gewichte des Modells zu aktualisieren, werden zunächst die Gradienten des Optimierers auf Null gesetzt (```.zero_grad()```), dann der Backpropagation-Schritt (```loss.backward()```) durchgeführt und schließlich mit ```optimizer.step()``` die Parameter aktualisiert.\n",
    "    \n",
    "    - Der Verlust des Batches wird akkumuliert.\n",
    "\n",
    "3. Berechnung des durchschnittlichen Verlusts:\n",
    "\n",
    "    - Nach Durchlauf aller Batches wird der durchschnittliche Verlust der Epoche berechnet und in einer Historienliste (```loss_history```) gespeichert.\n",
    "\n",
    "4. Monitoring:\n",
    "\n",
    "    - Alle 100 Epochen (oder in einem anderen gewünschten Intervall) wird der durchschnittliche Verlust ausgegeben, um den Trainingsfortschritt zu überwachen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A11: Training des Modells\n",
    "loss_history = []  # zum Speichern des durchschnittlichen Verlusts pro Epoche\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Vorwärtsdurchlauf\n",
    "        outputs = #...\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backpropagation und Optimierung\n",
    "        #... Gradienten auf null setzen\n",
    "        #... Backpropagation\n",
    "        #... Parameter aktualisieren\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    #... avg_loss zur loss_history hinzufügen\n",
    "    \n",
    "    # Ausgabe des Verlusts alle 100 Epochen\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Modell evaluieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach dem Training wird das Modell in den Evaluierungsmodus versetzt (```mit model.eval()```). Unter Verwendung von ```torch.no_grad()``` werden Vorhersagen für Test- und Validierungsdaten gemacht. Die Ausgaben werden in Klassen umgewandelt (Schwellenwert 0.5) und gängige Metriken (Accuracy, Precision, Recall) berechnet und ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A12: Modell evaluieren (Testdaten und Validierungsdaten)\n",
    "\n",
    "model.eval()  # Setze das Modell in den Evaluierungsmodus\n",
    "with torch.no_grad():\n",
    "    # Vorhersagen auf den Testdaten\n",
    "    y_pred_test = model(X_test_tensor)\n",
    "    predicted_classes_test = (y_pred_test > 0.5).float()\n",
    "\n",
    "    # Vorhersagen auf den Validierungsdaten\n",
    "    y_pred_val = model(X_val_tensor)\n",
    "    predicted_classes_val = (y_pred_val > 0.5).float()\n",
    "\n",
    "    # Metriken für Testdaten berechnen\n",
    "    accuracy_test = #...\n",
    "    precision_test = #...\n",
    "    recall_test = #...\n",
    "\n",
    "    # Metriken für Validierungsdaten berechnens\n",
    "    accuracy_val = #...\n",
    "    precision_val = #...\n",
    "    recall_val = #...\n",
    "\n",
    "    print(f'\\nTestdaten Metriken:')\n",
    "    print(f'Accuracy: {accuracy_test:.6f}')\n",
    "    print(f'Precision: {precision_test:.6f}')\n",
    "    print(f'Recall: {recall_test:.6f}')\n",
    "\n",
    "    print(f'\\nValidierungsdaten Metriken:')\n",
    "    print(f'Accuracy: {accuracy_val:.6f}')\n",
    "    print(f'Precision: {precision_val:.6f}')\n",
    "    print(f'Recall: {recall_val:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5 Ergebnisse visualisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stelle den Verlauf der ```loss_history```grafisch dar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Hinweis</b></summary>\n",
    "  <p>Nutze Matplotlib (<code>plt</code>) oder Seaborn (<code>sns</code>) Plots um den Verlauf des loss über Zeit darzustellen.</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A13: Visualisierung: Trainingsverlust über Epochen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Hinweis</b></summary>\n",
    "  <p><code>y_val</code>: Enthält die \"Ground Truth\"</p>\n",
    "  <p><code>df_val_original</code>: Soll Original- und Vorhersagedaten enthalten und angezeigt werden</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A14: Vergleiche die Ergebnisse mit den originalen Daten\n",
    "\n",
    "original_feature_names = ['age', 'sex', 'cp', 'trestbps', 'chol', \n",
    "                          'fbs', 'restecg', 'thalach', 'exang', \n",
    "                          'oldpeak', 'slope', 'ca', 'thal']\n",
    "\n",
    "# Ursprüngliche Feature-Daten der Validierungsdaten zurücktransformieren\n",
    "X_val_original = scaler.inverse_transform(X_val_scaled)\n",
    "\n",
    "# DataFrame mit den ursprünglichen Feature-Daten und den originalen Feature-Namen\n",
    "df_val_original = pd.DataFrame(X_val_original, columns=original_feature_names)\n",
    "\n",
    "# Füge die Ground Truth hinzu\n",
    "#...\n",
    "\n",
    "# Berechne Vorhersagen für die Validierungsdaten (sofern noch nicht geschehen)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_val = model(X_val_tensor)\n",
    "    predicted_classes_val = (y_pred_val > 0.5).float()\n",
    "\n",
    "# Füge die Vorhersageergebnisse hinzu\n",
    "df_val_original[\"Predicted Probability\"] = y_pred_val.numpy().flatten()\n",
    "df_val_original[\"Predicted Class\"] = predicted_classes_val.numpy().flatten()\n",
    "\n",
    "# Zeige die ersten Zeilen der kombinierten Tabelle an\n",
    "display() # <- Daten die angezeigt werden sollen einfügen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
