{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Herz Erkrankungen - Wer ist erkrankt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![heart_desease.png](heart_desease.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bibliotheken importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas seaborn matplotlib numpy torch sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Daten laden\n",
    "\n",
    "Datenquelle: https://archive.ics.uci.edu/dataset/45/heart+disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"heart.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Erster Einblick in Daten\n",
    "\n",
    "- gewinne eine erste √úbersicht √ºber die Date\n",
    "- schaue, ob eventuell Daten fehlen\n",
    "- wie sind die Daten verteilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Hinweis</b></summary>\n",
    "  <p><code>.head(n)</code> (n=Anzahl der Spalten): zeigt die ersten n Spalten des DataFrames</p>\n",
    "  <p><code>.info()</code>: √úbersicht des DataFrames (Anzahl der Eintr√§ge, Spaltennamen, Datentypen, Speicherverbrauch)</p>\n",
    "  <p><code>.describe()</code>: statistische √úbersicht f√ºr numerische Spalten (Mittelwert, Standardabweichung, Minimum, Maximum)</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1: Schaue dir die Metainformationen an\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2: Schaue dir die Statistik der Daten an\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Hinweis</b></summary>\n",
    "  <p><code>.head(n)</code> (n=Anzahl der Spalten): zeigt die ersten n Spalten des DataFrames</p>\n",
    "  <p><code>.info()</code>: √úbersicht des DataFrames (Anzahl der Eintr√§ge, Spaltennamen, Datentypen, Speicherverbrauch)</p>\n",
    "  <p><code>.describe()</code>: statistische √úbersicht f√ºr numerische Spalten (Mittelwert, Standardabweichung, Minimum, Maximum)</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenbeschreibung\n",
    "\n",
    "| Feature   | Description | Values |\n",
    "|-----------|-------------|--------|\n",
    "| age       | Age in years | - |\n",
    "| sex       | Sex | 1 = male, 0 = female |\n",
    "| cp        | Chest pain type | 0 = typical angina, 1 = atypical angina, 2 = non-anginal pain, 3 = asymptomatic |\n",
    "| trestbps  | Resting blood pressure (in mm Hg on admission to the hospital) | - |\n",
    "| chol      | Serum cholesterol in mg/dl | - |\n",
    "| fbs       | Fasting blood sugar > 120 mg/dl | 1 = true, 0 = false |\n",
    "| restecg   | Resting electrocardiographic results | 0 = normal, 1 = ST-T wave abnormality, 2 = probable or definite left ventricular hypertrophy by Estes' criteria |\n",
    "| thalach   | Maximum heart rate achieved | - |\n",
    "| exang     | Exercise-induced angina | 1 = yes, 0 = no |\n",
    "| oldpeak   | ST depression induced by exercise relative to rest | - |\n",
    "| slope     | Slope of the peak exercise ST segment | 0 = upsloping, 1 = flat, 2 = downsloping |\n",
    "| ca        | Number of major vessels colored by fluoroscopy | 0‚Äì3 |\n",
    "| thal      | Thallium stress test result | 0 = error/NaN, 1 = fixed defect, 2 = normal, 3 = reversible defect |\n",
    "| target    | Diagnosis of heart disease | 0 = no disease, 1 = disease |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Folgerung</b></summary>\n",
    "  <ul>\n",
    "    <li><b>data[\"ca\"]:</b> enth√§lt Werte > 3, die inkorrekt sind. Diese Werte sollten entfernt werden.</li>\n",
    "    <li><b>data[\"thal\"]:</b> enth√§lt Werte = 0, die inkorrekt sind. Diese Werte sollten ebenfalls entfernt werden.</li>\n",
    "  </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['ca'] < 4]\n",
    "data = data[data['thal'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wissenseinschub - Thema und Daten verstehen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Informationsraster</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            display: flex;\n",
    "            justify-content: center;\n",
    "            align-items: center;\n",
    "            background-color: #f4f4f4;\n",
    "            margin: 0;\n",
    "            padding: 5vw; /* Dynamischer Abstand basierend auf der Bildschirmbreite */\n",
    "            box-sizing: border-box;\n",
    "            color: black;\n",
    "        }\n",
    "        .grid-container {\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(2, 1fr);\n",
    "            grid-template-rows: repeat(3, 1fr);\n",
    "            gap: 5vw; /* Dynamischer Abstand zwischen den Grid-Items */\n",
    "            width: 100%;\n",
    "            max-width: 100%; /* Container passt sich der Bildschirmbreite an */\n",
    "            padding: 2vw; /* Dynamischer Innenabstand basierend auf der Bildschirmbreite */\n",
    "            box-sizing: border-box;\n",
    "        }\n",
    "        .grid-item {\n",
    "            background: white;\n",
    "            padding: 2vw; /* Dynamischer Innenabstand basierend auf der Bildschirmbreite */\n",
    "            border-radius: 10px;\n",
    "            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
    "            text-align: center;\n",
    "            display: flex;\n",
    "            flex-direction: column;\n",
    "            justify-content: space-between;\n",
    "        }\n",
    "        .grid-item img {\n",
    "            width: 100%; /* Bildbreite passt sich der Breite des Grid-Items an */\n",
    "            height: auto; /* H√∂he wird automatisch angepasst, um das Seitenverh√§ltnis beizubehalten */\n",
    "            object-fit: contain; /* Bild wird skaliert, um den Container zu f√ºllen, ohne zugeschnitten zu werden */\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        h3 {\n",
    "            margin-bottom: 2vw; /* Dynamischer Abstand basierend auf der Bildschirmbreite */\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <div class=\"grid-container\">\n",
    "        <div class=\"grid-item\">\n",
    "            <h3>Blutdruck - trestbps</h3>\n",
    "            <p>In diesem Datensatz ist nur \n",
    "                <a href=\"https://www.verywellhealth.com/systolic-and-diastolic-blood-pressure-1746075\">\n",
    "                    systolischer Blutdruck\n",
    "                </a> \n",
    "                enthalten.\n",
    "            </p>\n",
    "            <img src=\"Blutdruck.jpg\" alt=\"Blutdruck\" style=\"height: 40vw;\">\n",
    "        </div>\n",
    "        <div class=\"grid-item\">\n",
    "            <h3>Cholesterin - chol</h3>\n",
    "            <p>LDL (schlechtes Cholesterin), HDL (gutes Cholisterin)</p>\n",
    "            <img src=\"cholesterin.jpeg\" alt=\"Cholesterin\">\n",
    "        </div>\n",
    "        <div class=\"grid-item\">\n",
    "            <h3>Brustschmerzen - cp</h3>\n",
    "            <p>typische Angia (0): Druck/Engegef√ºhl in Brust; meist bei Anstrengung; bessert sich in Ruhe</p>\n",
    "            <p>atypische Angia (1): ungew√∂hnliche Symptome; Schmerzen an anderen Stellen oder in Ruhe</p>\n",
    "            <p>nicht-angin√∂ser Schmerz (2): Schmerzen, die nicht auf Herz zur√ºckzuf√ºhren sind; Muskel-/Verdauungsprobleme</p>\n",
    "            <p>asymptomatisch (3): keine sp√ºrbaren Brustschmerzen; oft bei stillen Herzroblemen</p>\n",
    "            <img src=\"angina.png\" alt=\"Angina\">\n",
    "        </div>\n",
    "        <div class=\"grid-item\">\n",
    "            <h3>Ruhe-Elektrodiografie (EKG) - restecg</h3>\n",
    "            <p>normal (0): keine Auff√§lligkeiten; Herzrhytmus und elektrische Aktivit√§t im Normalbereich</p>\n",
    "            <p>ST-T-Wellen-Abnomalie (1): Ver√§nderung in ST- oder T-Wellen; deutet auf Isch√§mie (unzureichende Blutversorgung des Herzmuskels), Elektrolytst√∂rung oder andere kardiale Probleme hin</p>\n",
    "            <p>wahrscheinliche oder definitive linksventrikul√§re Hypertrophie nach Estes¬¥ Kriterien (2): Merkmale in EKG deuten auf Verdickung linker Herzkammer hin; oft durch Bluthochdruck oder andere herzerkrankungen</p>\n",
    "            <img src=\"EKG.png\" alt=\"EKG\">\n",
    "        </div>\n",
    "        <div class=\"grid-item\">\n",
    "            <h3>Fluoroskopie - ca</h3>\n",
    "            <p>Kontrastmittel in Hauptgef√§√üen um Verengungen oder Blockaden zu identifizieren</p>\n",
    "            <img src=\"Fluoroscopy.png\" alt=\"Fluoroskopie\">\n",
    "        </div>\n",
    "        <div class=\"grid-item\">\n",
    "            <h3>Thalium-Stresstest - thal</h3>\n",
    "            <p>Fehler/NaN (0)</p>\n",
    "            <p>fester Defekt (1): in Ruhe und Belastund verminderte Durchblutung; deutet auf Narbengewebe durch fr√ºheren Herzinfarkt hin</p>\n",
    "            <p>Normal (2)</p>\n",
    "            <p>reversibler Defekt (3): ein Bereich des Herzmuskels unter Belastung verminderte Durchblutung (in Ruhe normalisiert); oft durch Verengung der Koronararterie</p>\n",
    "            <img src=\"Thalium.jpeg\" alt=\"Thalium-Test\">\n",
    "        </div>\n",
    "    </div>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gewinne einen detailierten Einblick in die Daten\n",
    "- stelle Verteilungen, Au√ürei√üer und Korrelationen grafisch dar\n",
    "\n",
    "Dokumentation Matplotlib: https://matplotlib.org/stable/plot_types/index </br>\n",
    "Dokumentation Seaborn: https://seaborn.pydata.org/examples/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Verteilung von Werten einzelner Features: </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Hinweis zur Datenvisualisierung</b></summary>\n",
    "  <br>\n",
    "  In der Datenanalyse ist es wichtig, zwischen kategorialen und kontinuierlichen Features zu unterscheiden, da sie unterschiedliche Visualisierungstechniken erfordern:\n",
    "\n",
    "  - **Kategoriale Daten** sind Daten, die in Gruppen oder Kategorien unterteilt sind, z. B. Geschlecht (m√§nnlich/weiblich) oder Farbe (rot/blau/gr√ºn). Diese Daten k√∂nnen als diskrete Werte betrachtet werden.\n",
    "  - **Kontinuierliche Daten** sind Daten, die in einem kontinuierlichen Bereich existieren, z. B. Temperatur oder Gewicht. Sie k√∂nnen jeden Wert innerhalb eines Intervalls annehmen und werden oft auf einer kontinuierlichen Skala dargestellt.\n",
    "\n",
    "  **Warum diese Unterscheidung wichtig ist:**\n",
    "  - Kategoriale und kontinuierliche Daten haben unterschiedliche Eigenschaften und erfordern daher unterschiedliche Visualisierungsmethoden, um Muster und Zusammenh√§nge klar darzustellen.\n",
    "  \n",
    "  **Visualisierung mit Seaborn und Matplotlib:**\n",
    "\n",
    "  - **F√ºr kategoriale Daten:**\n",
    "    - **Piecharts** sind n√ºtzlich, um die prozentuale Verteilung von Kategorien darzustellen. Sie zeigen, wie sich verschiedene Kategorien im Verh√§ltnis zueinander verhalten.\n",
    "      - Beispiel mit Matplotlib:\n",
    "        ```python\n",
    "        import matplotlib.pyplot as plt\n",
    "        data = ['Kategorie A', 'Kategorie B', 'Kategorie C']\n",
    "        values = [30, 40, 30]\n",
    "        plt.pie(values, labels=data, autopct='%1.1f%%', startangle=90)\n",
    "        plt.title('Verteilung der Kategorien')\n",
    "        plt.show()\n",
    "        ```\n",
    "      - Beispiel mit Seaborn (f√ºr eine √§hnliche Darstellung):\n",
    "        ```python\n",
    "        import seaborn as sns\n",
    "        sns.countplot(x='Kategorie', data=df)\n",
    "        plt.title('Verteilung der Kategorien')\n",
    "        plt.show()\n",
    "        ```\n",
    "\n",
    "  - **F√ºr kontinuierliche Daten:**\n",
    "    - **Distplots (Verteilungsdiagramme)** und **Boxplots** sind hilfreich, um die Verteilung, Ausrei√üer und zentrale Tendenzen von kontinuierlichen Daten zu visualisieren.\n",
    "      - **Distplot** (mit optionaler KDE) zeigt die H√§ufigkeit der Werte und deren Verteilung:\n",
    "        ```python\n",
    "        import seaborn as sns\n",
    "        sns.histplot(df['Wert'], kde=True)\n",
    "        plt.title('Verteilung der kontinuierlichen Daten')\n",
    "        plt.show()\n",
    "        ```\n",
    "      - **Boxplot** zeigt die quartilebasierte Verteilung und identifiziert Ausrei√üer:\n",
    "        ```python\n",
    "        sns.boxplot(x=df['Wert'])\n",
    "        plt.title('Boxplot der kontinuierlichen Daten')\n",
    "        plt.show()\n",
    "        ```\n",
    "\n",
    "  <br>\n",
    "  <b>Zusammenfassung:</b>\n",
    "  <ul>\n",
    "    <li>Kategoriale Daten sind f√ºr Piecharts oder Countplots geeignet, um die H√§ufigkeit der Kategorien darzustellen.</li>\n",
    "    <li>Kontinuierliche Daten werden am besten mit Distplots oder Boxplots visualisiert, um die Verteilung, Ausrei√üer und zentrale Tendenzen zu zeigen.</li>\n",
    "  </ul>\n",
    "\n",
    "  <br><img src=\"dist-and-outliers.png\" alt=\"Verteilung und Ausrei√üer\" style=\"max-width: 100%; height: auto;\"/>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A3: Definiere 2 Arrays, die kategeriale und kontinuierliche Features umfassen\n",
    "\n",
    "categorical_features = \n",
    "continuous_features = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A4: Stelle die Verteilung der Features dar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>üìå Wichtige Erkenntnisse</b></summary>\n",
    "  <br>\n",
    "  <ul>\n",
    "    <li><b>Ungleichverteilung der Daten:</b> \n",
    "      <br> Sowohl kategoriale als auch kontinuierliche Features sind nicht gleichm√§√üig verteilt. \n",
    "      <br> Dies kann das Training neuronaler Netzwerke beeinflussen.</li>\n",
    "    <br>\n",
    "    <li><b>Neuronale Netzwerke sind skalenempfindlich:</b> \n",
    "      <br> Unterschiedliche Wertebereiche der Features k√∂nnen das Lernen erschweren. \n",
    "      <br><b>‚ûú L√∂sung:</b> Standardisierung oder Normalisierung der Daten.</li>\n",
    "    <br>\n",
    "    <li><b>Ausrei√üer in kontinuierlichen Features:</b> \n",
    "      <br> Werte au√üerhalb des folgenden Bereichs gelten als Ausrei√üer:\n",
    "      <ul>\n",
    "        <li><b>Untere Grenze:</b> Q1 ‚àí 1.5 √ó IQR</li>\n",
    "        <li><b>Obere Grenze:</b> Q3 + 1.5 √ó IQR</li>\n",
    "      </ul>\n",
    "      <br><b>‚ûú Erkl√§rung:</b> Der Interquartilsabstand (IQR) beschreibt die mittleren 50 % der Daten.\n",
    "      Werte au√üerhalb dieses Bereichs sind potenzielle Ausrei√üer.</li>\n",
    "  </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Korrelation und Zusammenhang zwischen Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Hinweis</b></summary>\n",
    "  <p><code>.corr()</code>: Berechnet <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\">Korrelationsmatrix</A> aller Features</p>\n",
    "  <p><code>sns.heatmap()</code>: Eine <a href=\"https://seaborn.pydata.org/generated/seaborn.heatmap.html\"> Heatmap </a> bietet einen intuitiven und schnellen Einblick in Werte mittels Farbcodierung</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A5: Stelle die Korrelation zwischen Target und den anderen Features grafisch dar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Datenaufbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Daten aufteilen\n",
    "\n",
    "Zuerst m√ºssen die Daten in die Merkmale (X) und die Zielvariable (y) aufgeteilt werden. </br>\n",
    "Um das Model sp√§ter trainieren zu k√∂nnen und unser Ergebnis, an dem Model unbekannten Daten, zu testen m√ºssen die Daten in Trainings-, Test- und Validierungsdaten aufgeteilt werden.\n",
    "Zuerst die Aufteilung in Trainings und Testdaten um danach aus den Trainingsdaten Validierungsdaten zu extrahieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Hinweis</b></summary>\n",
    "  <p><code>.drop(\"target\")</code>: Entfernt die Variable \"target\" aus den Features</p>\n",
    "  <p><code>df[\"target\"]</code>: Gibt die Spalte \"target\" zur√ºck</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A6: Unterteile die Daten in Merkmale (X) und Zielvariable (y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Hinweis</b></summary>\n",
    "  <p><code>train_test_split(dataframe1, dateframe2, test_size=0.x, random_stat=42)</code>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">train_test_split</a> teilt die Dataframes nach mit der angegebenen Aufteilung auf</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A7: Unterteile X und y in train, test und val Daten\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Daten skalieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Skalierung von Daten und der StandardScaler</b></summary>\n",
    "  <p>Die <b>Skalierung</b> von Daten ist ein entscheidender Schritt in der Vorverarbeitung von Daten f√ºr maschinelles Lernen. Sie sorgt daf√ºr, dass alle Merkmale (Features) auf einem √§hnlichen Wertebereich liegen, was f√ºr viele Algorithmen wichtig ist, da diese oft empfindlich gegen√ºber der Skala der Eingabedaten sind.</p>\n",
    "\n",
    "  <p><b>Warum Skalierung notwendig ist:</b></p>\n",
    "  <ul>\n",
    "    <li>Algorithmen wie <i>Gradient Descent</i> und <i>k-NN</i> sind empfindlich gegen√ºber der Skala der Eingabedaten. Wenn die Features unterschiedliche Skalen haben (z. B. ein Feature von 0 bis 1 und ein anderes von 0 bis 1000), kann das Modell Schwierigkeiten haben, die Daten zu verarbeiten und zu lernen.</li>\n",
    "    <li>Skalierung sorgt daf√ºr, dass alle Features einen √§hnlichen Einfluss auf das Modell haben, was das Training stabiler und schneller macht.</li>\n",
    "    <li>Beispiel: Wenn eines der Features eine viel gr√∂√üere Range hat, k√∂nnte das Modell dazu neigen, diesem Feature mehr Gewicht zu geben, obwohl es m√∂glicherweise weniger wichtig ist.</li>\n",
    "  </ul>\n",
    "\n",
    "  <p><b>Was bewirkt die Skalierung?</b></p>\n",
    "  <ul>\n",
    "    <li>Die Skalierung ver√§ndert die Daten so, dass sie in einem bestimmten Bereich liegen oder eine bestimmte Verteilung haben. H√§ufig verwendete Skalierungsans√§tze sind die Standardisierung und Min-Max-Skalierung.</li>\n",
    "    <li>Durch Skalierung wird der Mittelwert eines Features auf einen bestimmten Wert (z. B. 0) gesetzt und die Standardabweichung auf einen bestimmten Wert (z. B. 1), was die Daten zentriert und skaliert.</li>\n",
    "  </ul>\n",
    "\n",
    "  <p><b>Wie funktioniert der StandardScaler?</b></p>\n",
    "  <p>Der <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\" target=\"_blank\"><b>StandardScaler</b></a> von <i>scikit-learn</i> standardisiert die Features, indem er den Mittelwert (Durchschnitt) jedes Features auf 0 und die Standardabweichung auf 1 setzt. Die Formel lautet:</p>\n",
    "\n",
    "  <pre>\n",
    "    Z = (X - Œº) / œÉ\n",
    "  </pre>\n",
    "  <p>Dabei ist:</p>\n",
    "  <ul>\n",
    "    <li><b>X</b>: Der urspr√ºngliche Wert des Features.</li>\n",
    "    <li><b>Œº</b>: Der Mittelwert des Features.</li>\n",
    "    <li><b>œÉ</b>: Die Standardabweichung des Features.</li>\n",
    "    <li><b>Z</b>: Der standardisierte Wert des Features.</li>\n",
    "  </ul>\n",
    "\n",
    "  <p>Der StandardScaler berechnet den Mittelwert und die Standardabweichung des Trainingsdatensatzes und wendet dann diese Werte auf die Trainings-, Validierungs- und Testdaten an (wobei er die Testdaten nur transformiert, nicht anpasst, um Datenleakage zu vermeiden).</p>\n",
    "\n",
    "  <p>Die Vorteile des StandardScalers sind, dass er die Daten zentriert und skaliert, was insbesondere f√ºr Modelle wie <i>lineare Regression</i>, <i>Support Vector Machines</i> und neuronale Netze von Bedeutung ist.</p>\n",
    "\n",
    "  <p>Weitere Informationen findest du in der <a href=\"https://scikit-learn.org/stable/modules/preprocessing.html\" target=\"_blank\">offiziellen Dokumentation von scikit-learn</a>.</p>\n",
    "\n",
    "  <br>\n",
    "  <img src=\"scaling.png\" alt=\"Scaling vorher vs. danach\" style=\"max-width: 100%; height: auto;\"/>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A8: Skaliere die Daten (X_train: fit_transform, X_val, X_test: transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model trainieren und auswerten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Datenvorbereitung:** Konvertierung von bereits skalierten Daten in PyTorch-Tensoren.\n",
    "2. **Modell-Definition:** Aufbau eines simplen Netzwerks mit zwei verborgenen Schichten.\n",
    "3. **Training:** Training des Netzwerks √ºber 2500 Epochen mit Aufzeichnung des Trainingsverlusts.\n",
    "4. **Evaluation:** Berechnung von Accuracy, Precision und Recall auf den Validierungs- und Testdaten.\n",
    "5. **Visualisierung:** Darstellung des Trainingsverlaufs und der Evaluationsmetriken als Grafiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter definieren\n",
    "batch_size = 32\n",
    "input_size = 13  # Anzahl der Eingabefeatures\n",
    "output_size = 1  # Nur eine Ausgabe (Krankheit: Ja/Nein)\n",
    "hidden_layers = [32, 16]  # Zwei verborgene Schichten\n",
    "epochs = 1000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Daten in PyTorch-Tensoren umwandeln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ausgehend davon, dass die Daten bereits vorverarbeitet (z.‚ÄØB. skaliert) vorliegen, m√ºssen wir sie in das Format bringen, das PyTorch f√ºr das Training ben√∂tigt. Dabei erfolgt Folgendes:\n",
    "\n",
    "- Tensor-Konvertierung: Die Feature-Matrizen und Zielvariablen werden in PyTorch-Tensoren umgewandelt. Dabei wird der Datentyp auf ```float32``` festgelegt. F√ºr die Zielvariablen wird zus√§tzlich die Dimension angepasst, sodass sie als Spaltenvektor vorliegen (mittels ```.view(-1, 1)```).\n",
    "\n",
    "- Erstellung eines Datasets: Mit ```TensorDataset``` werden die zugeh√∂rigen Feature- und Ziel-Tensoren zusammengef√ºhrt, sodass jeder Eintrag im Dataset ein Tupel aus (Feature, Ziel) darstellt.\n",
    "\n",
    "- Erstellung eines DataLoaders: Mittels ```DataLoader``` wird das Dataset in Batches unterteilt, um das Training zu erleichtern. Beim Trainings-Dataloader wird ```shuffle=True``` verwendet, um die Daten vor jedem Trainingsepochendurchlauf zuf√§llig zu mischen ‚Äì dies hilft, die Generalisierungsf√§higkeit des Modells zu verbessern. F√ºr den Validierungs- (und Test-) Dataloader wird ```shuffle=False``` gew√§hlt, um die Reihenfolge der Daten beizubehalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A9: Wandle die Daten in Pytorch-Tensoren um\n",
    "\n",
    "# Hier gehe ich davon aus, dass die Daten bereits skaliert sind\n",
    "X_train_tensor =\n",
    "y_train_tensor =\n",
    "\n",
    "X_val_tensor =\n",
    "y_val_tensor =\n",
    "\n",
    "X_test_tensor = \n",
    "y_test_tensor =\n",
    "\n",
    "# Erstelle DataLoader f√ºr das Training und die Validierung\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Modell definieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Das SimpleNN-Modell besteht aus einer Eingabeschicht, zwei verborgenen Schichten mit ReLU-Aktivierung und einer Ausgabeschicht, die eine Sigmoid-Aktivierung verwendet (ideal f√ºr bin√§re Klassifikation).\n",
    "<details>\n",
    "<summary><b>Hinweis</b></summary>\n",
    "<p><a href=\"https://playground.tensorflow.org/\">Hier</a> kannst du dier anschauen wie dein Netzwerk aussehen k√∂nnte un was die Parameter bewirken.</p>\n",
    "<p><a href=\"https://yajm.medium.com/simple-neural-network-in-pytorch-step-by-step-guide-fe12bdbbae33\">Hier</a> findest du eine simple Dokumentation zum erstellen eines neuronalen Netzwerkes.</p>\n",
    "</br>\n",
    "<p>Dies ist ein m√∂glicher Aufbau des Netzwekrs:</p>\n",
    "<img src=\"simplenn_arch.png\" alt=\"SimpleNN Architektur\" style=\"max-width: 100%; height: auto;\"/>\n",
    "\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A10: Definiere das Modell\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        # Eingabeschicht\n",
    "        #...\n",
    "        # Verborgene Schichten\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            #...\n",
    "        # Ausgabeschicht\n",
    "        #...\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Modell instanziieren\n",
    "model = SimpleNN(input_size, hidden_layers, output_size)\n",
    "\n",
    "# Verlustfunktion und Optimierer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss f√ºr bin√§re Klassifikation\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Modell trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Code f√ºhrt einen Trainingszyklus √ºber eine festgelegte Anzahl an Epochen durch. Dabei werden in jeder Epoche die folgenden Schritte durchgef√ºhrt:\n",
    "\n",
    "1. Moduswechsel und Initialisierung: \n",
    "    - Das Modell wird in den Trainingsmodus versetzt (mit ```model.train()```), damit alle Schichten (z.‚ÄØB. Dropout, BatchNorm) korrekt arbeiten. \n",
    "    - Eine Variable (```running_loss```) wird initialisiert, um den kumulativen Verlust der aktuellen Epoche zu speichern.\n",
    "\n",
    "2. Batch-Weise Verarbeitung: \n",
    "    - F√ºr jeden Batch aus dem Trainings-Dataloader wird ein Vorw√§rtsdurchlauf ausgef√ºhrt, indem das Modell die Eingaben verarbeitet und Vorhersagen (```outputs```) liefert.\n",
    "\n",
    "    - Anschlie√üend wird der Verlust (Loss) zwischen den Vorhersagen und den tats√§chlichen Zielwerten berechnet.\n",
    "\n",
    "    - Um die Gewichte des Modells zu aktualisieren, werden zun√§chst die Gradienten des Optimierers auf Null gesetzt (```.zero_grad()```), dann der Backpropagation-Schritt (```loss.backward()```) durchgef√ºhrt und schlie√ülich mit ```optimizer.step()``` die Parameter aktualisiert.\n",
    "    \n",
    "    - Der Verlust des Batches wird akkumuliert.\n",
    "\n",
    "3. Berechnung des durchschnittlichen Verlusts:\n",
    "\n",
    "    - Nach Durchlauf aller Batches wird der durchschnittliche Verlust der Epoche berechnet und in einer Historienliste (```loss_history```) gespeichert.\n",
    "\n",
    "4. Monitoring:\n",
    "\n",
    "    - Alle 100 Epochen (oder in einem anderen gew√ºnschten Intervall) wird der durchschnittliche Verlust ausgegeben, um den Trainingsfortschritt zu √ºberwachen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A11: Training des Modells\n",
    "loss_history = []  # zum Speichern des durchschnittlichen Verlusts pro Epoche\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Vorw√§rtsdurchlauf\n",
    "        outputs = #...\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backpropagation und Optimierung\n",
    "        #... Gradienten auf null setzen\n",
    "        #... Backpropagation\n",
    "        #... Parameter aktualisieren\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    #... avg_loss zur loss_history hinzuf√ºgen\n",
    "    \n",
    "    # Ausgabe des Verlusts alle 100 Epochen\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Modell evaluieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach dem Training wird das Modell in den Evaluierungsmodus versetzt (```mit model.eval()```). Unter Verwendung von ```torch.no_grad()``` werden Vorhersagen f√ºr Test- und Validierungsdaten gemacht. Die Ausgaben werden in Klassen umgewandelt (Schwellenwert 0.5) und g√§ngige Metriken (Accuracy, Precision, Recall) berechnet und ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A12: Modell evaluieren (Testdaten und Validierungsdaten)\n",
    "\n",
    "model.eval()  # Setze das Modell in den Evaluierungsmodus\n",
    "with torch.no_grad():\n",
    "    # Vorhersagen auf den Testdaten\n",
    "    y_pred_test = model(X_test_tensor)\n",
    "    predicted_classes_test = (y_pred_test > 0.5).float()\n",
    "\n",
    "    # Vorhersagen auf den Validierungsdaten\n",
    "    y_pred_val = model(X_val_tensor)\n",
    "    predicted_classes_val = (y_pred_val > 0.5).float()\n",
    "\n",
    "    # Metriken f√ºr Testdaten berechnen\n",
    "    accuracy_test = #...\n",
    "    precision_test = #...\n",
    "    recall_test = #...\n",
    "\n",
    "    # Metriken f√ºr Validierungsdaten berechnens\n",
    "    accuracy_val = #...\n",
    "    precision_val = #...\n",
    "    recall_val = #...\n",
    "\n",
    "    print(f'\\nTestdaten Metriken:')\n",
    "    print(f'Accuracy: {accuracy_test:.6f}')\n",
    "    print(f'Precision: {precision_test:.6f}')\n",
    "    print(f'Recall: {recall_test:.6f}')\n",
    "\n",
    "    print(f'\\nValidierungsdaten Metriken:')\n",
    "    print(f'Accuracy: {accuracy_val:.6f}')\n",
    "    print(f'Precision: {precision_val:.6f}')\n",
    "    print(f'Recall: {recall_val:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5 Ergebnisse visualisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stelle den Verlauf der ```loss_history```grafisch dar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Hinweis</b></summary>\n",
    "  <p>Nutze Matplotlib (<code>plt</code>) oder Seaborn (<code>sns</code>) Plots um den Verlauf des loss √ºber Zeit darzustellen.</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A13: Visualisierung: Trainingsverlust √ºber Epochen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Hinweis</b></summary>\n",
    "  <p><code>y_val</code>: Enth√§lt die \"Ground Truth\"</p>\n",
    "  <p><code>df_val_original</code>: Soll Original- und Vorhersagedaten enthalten und angezeigt werden</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A14: Vergleiche die Ergebnisse mit den originalen Daten\n",
    "\n",
    "original_feature_names = ['age', 'sex', 'cp', 'trestbps', 'chol', \n",
    "                          'fbs', 'restecg', 'thalach', 'exang', \n",
    "                          'oldpeak', 'slope', 'ca', 'thal']\n",
    "\n",
    "# Urspr√ºngliche Feature-Daten der Validierungsdaten zur√ºcktransformieren\n",
    "X_val_original = scaler.inverse_transform(X_val_scaled)\n",
    "\n",
    "# DataFrame mit den urspr√ºnglichen Feature-Daten und den originalen Feature-Namen\n",
    "df_val_original = pd.DataFrame(X_val_original, columns=original_feature_names)\n",
    "\n",
    "# F√ºge die Ground Truth hinzu\n",
    "#...\n",
    "\n",
    "# Berechne Vorhersagen f√ºr die Validierungsdaten (sofern noch nicht geschehen)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_val = model(X_val_tensor)\n",
    "    predicted_classes_val = (y_pred_val > 0.5).float()\n",
    "\n",
    "# F√ºge die Vorhersageergebnisse hinzu\n",
    "df_val_original[\"Predicted Probability\"] = y_pred_val.numpy().flatten()\n",
    "df_val_original[\"Predicted Class\"] = predicted_classes_val.numpy().flatten()\n",
    "\n",
    "# Zeige die ersten Zeilen der kombinierten Tabelle an\n",
    "display() # <- Daten die angezeigt werden sollen einf√ºgen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
